{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate RAG with LlamaIndex Locally"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> 注意事项：\n",
        "> 1. 我用 LM Studio 模拟 OpenAPI API 调用。\n",
        "> 2. 本次使用的 LLM 模型为 mistral-7b-instruct-v0.2.Q6_K。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SxQ2qzb7DPu1"
      },
      "source": [
        "下面，我将探讨如何构建一个检索增强生成（Retrieval-Augmented Generation, RAG）的流程，并利用 LlamaIndex 对该流程进行评估。文档内容涵盖以下三大部分：\n",
        "\n",
        "1. 理解检索增强生成（RAG）。\n",
        "2. 利用 LlamaIndex 构建 RAG 流程。\n",
        "3. 利用 LlamaIndex 对 RAG 进行评估。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **检索增强生成 (Retrieval-Augmented Generation, RAG)**\n",
        "\n",
        "大语言模型 (Large Language Models, LLM) 在庞大的数据集上训练，这些数据集往往不包含您个人的具体数据。检索增强生成技术 (RAG) 通过在生成过程中动态地结合用户数据，来弥补这一缺陷。重点在于，不是修改大语言模型的训练数据集，而是让模型能够实时接入并利用这些用户数据，从而提供更加定制化且与上下文相关的回答。\n",
        "\n",
        "在 RAG 系统中，首先要做的是加载用户数据并为查询“建立索引”。当用户发起查询时，系统会在索引中过滤，找出与查询最相关的上下文。接着，这些相关的上下文和用户问题将一起提交给大语言模型，模型便据此提供相应的答案。\n",
        "\n",
        "无论您打算构建的是聊天机器人还是自动应答代理，都需要掌握 RAG 技术，以便能够有效地将数据整合到您的应用中。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![RAG Overview](./images/llamaindex_rag_overview.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv9e2I1w-SLF"
      },
      "source": [
        "## **RAG 的关键阶段** \n",
        "\n",
        "RAG 包括五个关键阶段，这些都是构建任何大型应用程序不可或缺的一部分： \n",
        "1. **Loading加载：** 指的是将数据从其所在位置 — 如文本文件、PDF、其他网站、数据库或 API — 导入到您的处理流程中。LlamaHub 提供了数百种连接器，供您选择。 \n",
        "2. **Indexing索引：** 创建一个支持数据查询的结构。对于大语言模型而言，这通常涉及创建向量嵌入（即数据的数值化语义表示），以及其他多种元数据策略，以简化并提高寻找相关上下文数据的准确性。 \n",
        "3. **Storing存储：** 数据索引后，会希望存储该索引以及所有相关元数据，以省去重复索引的步骤。 \n",
        "4. **Querying查询：** 在任何已确定的索引策略下，您都可以利用大语言模型和 LlamaIndex 数据结构来执行多种查询方式，包括子查询、多步查询和混合查询策略。 \n",
        "5. **Evaluation评估：** 评估是流程中一个至关重要的步骤，它用于检查流程相较于其他策略的有效性或进行调整时的表现如何。评估为查询回应的准确性、一致性和速度提供了客观的量度标准。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Besynnjg_Cg9"
      },
      "source": [
        "# 代码实践：构建一个简单RAG系统，并且对其质量评估"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1NdWoBI_OFR"
      },
      "outputs": [],
      "source": [
        "# `nest_asyncio` 模块允许异步函数在一个已经启动的异步循环内部进行嵌套执行。\n",
        "# 这样做的必要性在于，Jupyter Notebook 这一工具天生就是在一个异步循环的环境下操作的。\n",
        "# 通过使用 `nest_asyncio`，我们能够顺利地在这一现有的异步循环中添加并运行更多的异步函数，而不会引起冲突。\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from llama_index.evaluation import generate_question_context_pairs\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.node_parser import SimpleNodeParser\n",
        "from llama_index.evaluation import generate_question_context_pairs\n",
        "from llama_index.evaluation import RetrieverEvaluator\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.embeddings import resolve_embed_model\n",
        "\n",
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7_DxX6xKUbH"
      },
      "source": [
        "#### 加载本地数据并构建索引"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vCJln_YKaK-",
        "outputId": "0004d366-35a3-49d4-806c-86a307aaa560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Pooling config file not found; pooling mode is defaulted to 'cls'.\n"
          ]
        }
      ],
      "source": [
        "# load data from data directory\n",
        "documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "\n",
        "# bge-m3 embedding model\n",
        "# https://huggingface.co/BAAI/bge-base-en-v1.5/tree/main\n",
        "embed_model = resolve_embed_model(\"local:BAAI/bge-base-en-v1.5\")\n",
        "\n",
        "# Load LM Studio LLM model\n",
        "llm = OpenAI(api_base=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n",
        "\n",
        "# Index the data\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    embed_model=embed_model, llm=llm,\n",
        ")\n",
        "\n",
        "# Transform data to Nodes struct\n",
        "node_parser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=128)\n",
        "nodes = node_parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# vetorize\n",
        "vector_index = VectorStoreIndex.from_documents(\n",
        "    documents, service_context=service_context\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65FyKpfY_inX"
      },
      "source": [
        "构建向量查询engine，并准备执行向量查询"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "kOZBy--R_m3I"
      },
      "outputs": [],
      "source": [
        "query_engine = vector_index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "G7NVP-N4_rXF"
      },
      "outputs": [],
      "source": [
        "response_vector = query_engine.query(\"What did the author do growing up?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPYx0ycS_x9B"
      },
      "source": [
        "检查输出结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "BFFLl1d7_4r4",
        "outputId": "74c2bc81-f326-4981-c7ac-92eb836e5e99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"The author grew up in New Hampshire and spent most of his time reading science fiction and painting. He attended a boarding school in Massachusetts for high school, where he continued to paint and read. After graduating from college with a degree in computer science, he worked at various software companies before starting his own company, Viaweb, which was sold to Yahoo in 1996. He then moved to California and tried to focus on painting, but found it difficult due to lack of energy and motivation. He eventually returned to New York and resumed painting, this time with more success.\\n### Explanation:\\nThe author's childhood was marked by a love for reading science fiction and painting. He attended a boarding school in Massachusetts, where he continued to pursue these interests. After college, he worked in the software industry before starting his own company, Viaweb, which was sold to Yahoo in 1996. Following the sale of Viaweb, the author moved to California with the intention of focusing on painting, but found it difficult due to a lack of energy and motivation. He eventually returned to New York and resumed painting with more success.\""
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response_vector.response"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IfhTYwoO_8cd"
      },
      "source": [
        "默认情况下，系统会检索出两个与查询内容相似的节点或数据块。你可以通过修改 `vector_index.as_query_engine(similarity_top_k=k)` 函数中的参数来调整检索相似节点的数量。\n",
        "\n",
        "我们接下来查看一下这些被检索出的节点中包含的文本内容。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcniGJVt_5V8",
        "outputId": "52b9d3da-30e7-460e-ab50-0ac3dc98ea1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\\n\\nI was puzzled by the 1401. I couldn\\'t figure out what to do with it. And in retrospect there\\'s not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn\\'t have any data stored on punched cards. The only other option was to do things that didn\\'t rely on any input, like calculate approximations of pi, but I didn\\'t know enough math to do anything interesting of that type. So I\\'m not surprised I can\\'t remember any programs I wrote, because they can\\'t have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn\\'t. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager\\'s expression made clear.\\n\\nWith microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then stopping. [1]\\n\\nThe first of my friends to get a microcomputer built it himself. It was sold as a kit by Heathkit. I remember vividly how impressed and envious I felt watching him sitting in front of it, typing programs right into the computer.\\n\\nComputers were expensive in those days and it took me years of nagging before I convinced my father to buy one, a TRS-80, in about 1980. The gold standard then was the Apple II, but a TRS-80 was good enough. This was when I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he\\'d write 2 pages at a time and then print them out, but it was a lot better than a typewriter.\\n\\nThough I liked programming, I didn\\'t plan to study it in college. In college I was going to study philosophy, which sounded much more powerful. It seemed, to my naive high school self, to be the study of the ultimate truths, compared to which the things studied in other fields would be mere domain knowledge. What I discovered when I got to college was that the other fields took up so much of the space of ideas that there wasn\\'t much left for these supposed ultimate truths. All that seemed left for philosophy were edge cases that people in other fields felt could safely be ignored.\\n\\nI couldn\\'t have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.\\n\\nAI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven\\'t tried rereading The Moon is a Harsh Mistress, so I don\\'t know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most.'"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# First retrieved node\n",
        "response_vector.source_nodes[0].get_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Nor had I changed my grad student lifestyle significantly since we started. So when Yahoo bought us it felt like going from rags to riches. Since we were going to California, I bought a car, a yellow 1998 VW GTI. I remember thinking that its leather seats alone were by far the most luxurious thing I owned.\\n\\nThe next year, from the summer of 1998 to the summer of 1999, must have been the least productive of my life. I didn't realize it at the time, but I was worn out from the effort and stress of running Viaweb. For a while after I got to California I tried to continue my usual m.o. of programming till 3 in the morning, but fatigue combined with Yahoo's prematurely aged culture and grim cube farm in Santa Clara gradually dragged me down. After a few months it felt disconcertingly like working at Interleaf.\\n\\nYahoo had given us a lot of options when they bought us. At the time I thought Yahoo was so overvalued that they'd never be worth anything, but to my astonishment the stock went up 5x in the next year. I hung on till the first chunk of options vested, then in the summer of 1999 I left. It had been so long since I'd painted anything that I'd half forgotten why I was doing this. My brain had been entirely full of software and men's shirts for 4 years. But I had done this to get rich so I could paint, I reminded myself, and now I was rich, so I should go paint.\\n\\nWhen I said I was leaving, my boss at Yahoo had a long conversation with me about my plans. I told him all about the kinds of pictures I wanted to paint. At the time I was touched that he took such an interest in me. Now I realize it was because he thought I was lying. My options at that point were worth about $2 million a month. If I was leaving that kind of money on the table, it could only be to go and start some new startup, and if I did, I might take people with me. This was the height of the Internet Bubble, and Yahoo was ground zero of it. My boss was at that moment a billionaire. Leaving then to start a new startup must have seemed to him an insanely, and yet also plausibly, ambitious plan.\\n\\nBut I really was quitting to paint, and I started immediately. There was no time to lose. I'd already burned 4 years getting rich. Now when I talk to founders who are leaving after selling their companies, my advice is always the same: take a vacation. That's what I should have done, just gone off somewhere and done nothing for a month or two, but the idea never occurred to me.\\n\\nSo I tried to paint, but I just didn't seem to have any energy or ambition. Part of the problem was that I didn't know many people in California. I'd compounded this problem by buying a house up in the Santa Cruz Mountains, with a beautiful view but miles from anywhere. I stuck it out for a few more months, then in desperation I went back to New York, where unless you understand about rent control you'll be surprised to hear I still had my apartment, sealed up like a tomb of my old life. Idelle was in New York at least, and there were other people trying to paint there, even though I didn't know any of them.\\n\\nWhen I got back to New York I resumed my old life, except now I was rich. It was as weird as it sounds. I resumed all my old patterns, except now there were doors where there hadn't been. Now when I was tired of walking, all I had to do was raise my hand, and (unless it was raining) a taxi would stop to pick me up. Now when I walked past charming little restaurants I could go in and order lunch. It was exciting for a while. Painting started to go better. I experimented with a new kind of still life where I'd paint one painting in the old way, then photograph it and print it, blown up, on canvas, and then use that as the underpainting for a second still life, painted from the same objects (which hopefully hadn't rotted yet).\\n\\nMeanwhile I looked for an apartment to buy. Now I could actually choose what neighborhood to live in. Where, I asked myself and various real estate agents, is the Cambridge of New York? Aided by occasional visits to actual Cambridge, I gradually realized there wasn't one. Huh.\\n\\nAround this time, in the spring of 2000, I had an idea. It was clear from our experience with Viaweb that web apps were the future.\""
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Second retrieved node\n",
        "response_vector.source_nodes[1].get_text()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnvSrTeADNrX"
      },
      "source": [
        "我们已经搭建了一个简单的 RAG 系统，并且现在我们需要对它的性能进行评价。可以通过运用 LlamaIndex 提供的核心评估模块对这个 RAG 系统或查询引擎进行评估。下面，我们探究如何使用这些工具定量衡量检索增强生成系统的质量。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jAMiOEsT_-o0"
      },
      "source": [
        "## Evaluation 评估\n",
        "\n",
        "评价工作应当成为衡量 RAG 应用表现的重要指标。这关乎系统针对不同数据源和多样的查询是否能够给出准确答案。\n",
        "\n",
        "起初，单独审查每一个查询和相应的响应有助于系统调优，但随着特殊情况和故障数量的增加，这种方式可能行不通。相比之下，建立一整套综合性评价指标或者自动化评估系统则更为高效。这类工具能够洞察系统整体性能并识别哪些领域需要进一步关注。\n",
        "\n",
        "RAG 系统的评估主要聚焦于两个核心方面：\n",
        "\n",
        "*   **检索评估：** 这是对系统检索出的信息的准确性与相关性进行评价的过程。\n",
        "*   **响应评估：** 这是基于检索结果对系统生成回答的质量和恰当性进行测量的过程。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "arGZqWQNIitt"
      },
      "source": [
        "### 生成 “问题-上下文” 对：\n",
        "\n",
        "在评价 RAG 系统时，关键在于有能力提出既能获取正确上下文，又能相应生成适当回答的问题。`LlamaIndex` 提供了一个 `generate_question_context_pairs` 模块，这个模块专门设计用来构建评价 RAG 系统的问题和上下文对，涵盖了检索评估和响应评估两大方面。如需了解更多关于问题生成的信息，请查阅[文档](https://docs.llamaindex.ai/en/stable/examples/evaluation/QuestionGeneration.html)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK0BWZ88LjDq",
        "outputId": "87d11e23-50cf-4ff3-a449-12264cfd8ab8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [03:53<00:00,  5.08s/it]\n"
          ]
        }
      ],
      "source": [
        "qa_dataset = generate_question_context_pairs(\n",
        "    nodes,\n",
        "    llm=llm,\n",
        "    num_questions_per_chunk=20\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjs2cgTpOfLM"
      },
      "source": [
        "### 检索评估：\n",
        "\n",
        "我们已经做好了开展检索评估的准备。我们将利用已生成的评估数据集来运行 `RetrieverEvaluator`。\n",
        "\n",
        "首先，我们需要建立一个 `Retriever` 实例，随后定义两个函数：`get_eval_results` 负责在数据集上执行检索操作，`display_results` 用于展现评估结果。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "fV9IdnwLM_aw"
      },
      "outputs": [],
      "source": [
        "retriever = vector_index.as_retriever(similarity_top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLSNg2sSPc2U"
      },
      "source": [
        "### 定义检索评估器：\n",
        "\n",
        "我们采用 **命中率 (Hit Rate)** 和 **平均倒数排名 (Mean Reciprocal Rank, MRR)** 这两项指标来对检索器进行评估。\n",
        "\n",
        "**命中率：**\n",
        "\n",
        "命中率衡量的是正确答案出现在检索结果前k个文档中的比例。换句话说，就是我们的系统在最开始的几次猜测中得到正确结果的频次。\n",
        "\n",
        "**平均倒数排名（MRR）：**\n",
        "\n",
        "MRR 通过分析最相关文档在检索结果里的排名来计算每个查询的准确性。更具体地说，它是所有查询的相关文档排名倒数的平均值。例如，若最相关的文档排在第一位，其倒数排名为 1；排在第二位时，为 1/2；以此类推。\n",
        "\n",
        "我们来通过这些指标来了解我们的检索器的表现。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "H6V_LCxrPQzp"
      },
      "outputs": [],
      "source": [
        "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
        "    [\"mrr\", \"hit_rate\"], retriever=retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYFgmnpRPX-x"
      },
      "outputs": [],
      "source": [
        "# Evaluate\n",
        "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3FLlvjoSbI5"
      },
      "source": [
        "定义一个函数，它可以将检索评估的结果以表格的形式展示出来。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "S9T268MhRNxp"
      },
      "outputs": [],
      "source": [
        "def display_results(name, eval_results):\n",
        "    \"\"\"Display results from evaluate.\"\"\"\n",
        "\n",
        "    metric_dicts = []\n",
        "    for eval_result in eval_results:\n",
        "        metric_dict = eval_result.metric_vals_dict\n",
        "        metric_dicts.append(metric_dict)\n",
        "\n",
        "    full_df = pd.DataFrame(metric_dicts)\n",
        "\n",
        "    hit_rate = full_df[\"hit_rate\"].mean()\n",
        "    mrr = full_df[\"mrr\"].mean()\n",
        "\n",
        "    metric_df = pd.DataFrame(\n",
        "        {\"Retriever Name\": [name], \"Hit Rate\": [hit_rate], \"MRR\": [mrr]}\n",
        "    )\n",
        "\n",
        "    return metric_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "A1eESYN-RRgl",
        "outputId": "ff27adb0-d189-4b7d-8998-6df15b6a2014"
      },
      "outputs": [],
      "source": [
        "display_results(\"bge-m3 Embedding Retriever\", eval_results)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 结果分析：\n",
        "\n",
        "结果显示 HitRate 比 MRR 的数值更大。\n",
        "MRR 的表现不如命中率意味着排名靠前的结果并不总是最匹配的。为了提升 MRR，可能需要引入重新排序器（rerankers），这些工具用于优化检索到的文档顺序。若想深入理解重新排序器如何精细调优检索指标，请参考我们在[博客文章](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)中的全面讲解。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPxAHE3kSjsT"
      },
      "source": [
        "## 质量评估工具：\n",
        "\n",
        "1. 忠实度评估器（FaithfulnessEvaluator）：这个工具用来衡量查询引擎的响应是否与其它已知的信息源相符合，能有效判断响应中是否包含了凭空捏造的内容。\n",
        "2. 相关度评估器（Relevancy Evaluator）：该评估器主要测量查询结果及其关联信息是否与用户的查询要求相匹配。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "-zMMJAQvRS8H"
      },
      "outputs": [],
      "source": [
        "# Get the list of queries from the above created dataset\n",
        "queries = list(qa_dataset.queries.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bNei9mj4UjN"
      },
      "source": [
        "### Faithfulness Evaluator\n",
        "\n",
        "我们先来看 Faithfulness 评估器："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "lrrD5n6w3Oet"
      },
      "outputs": [],
      "source": [
        "vector_index = VectorStoreIndex(nodes, service_context = service_context)\n",
        "query_engine = vector_index.as_query_engine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UvtdrdovgZ_"
      },
      "source": [
        "创建一个 FaithfulnessEvaluator 实例"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "dbvXvcFnU09s"
      },
      "outputs": [],
      "source": [
        "from llama_index.evaluation import FaithfulnessEvaluator\n",
        "faithfulness_evaluator = FaithfulnessEvaluator(service_context=service_context)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uA1hQ6_F4NoA"
      },
      "source": [
        "任意评估一个问题是否相关"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "9lfyhUuDz6cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Where did the writer have permission to use the IBM 1401 computer system?'"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_query = queries[3]\n",
        "eval_query"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "评估实例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "response_vector = query_engine.query(eval_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "MZ6lvmRf3j8i"
      },
      "outputs": [],
      "source": [
        "# Compute faithfulness evaluation\n",
        "eval_result = faithfulness_evaluator.evaluate_response(response=response_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj79Rq-gn3cv",
        "outputId": "5078aeb7-c620-45d6-dc1f-215e716f4e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You can check passing parameter in eval_result if it passed the evaluation.\n",
        "eval_result.passing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuVReAjp4eZ_"
      },
      "source": [
        "### Relevancy 相关度评估器\n",
        "\n",
        "相关度评估器（Relevancy Evaluator）非常适用于判断响应内容和提供的信息源（检索到的背景资料）是否对查询进行了准确的匹配。此工具能够帮助我们确认响应内容是否确实解答了用户的问题。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "Qw5X_hMB24kC"
      },
      "outputs": [],
      "source": [
        "from llama_index.evaluation import RelevancyEvaluator\n",
        "\n",
        "relevancy_evaluator = RelevancyEvaluator(service_context=service_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIl_JJhFvNhu"
      },
      "source": [
        "任意选择一个问题进行评估"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Where did the writer have permission to use the IBM 1401 computer system?'"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Pick a query\n",
        "query = queries[3]\n",
        "query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "r9FwcImG3cV0"
      },
      "outputs": [],
      "source": [
        "# Generate response.\n",
        "# response_vector has response and source nodes (retrieved context)\n",
        "response_vector = query_engine.query(query)\n",
        "\n",
        "# Relevancy evaluation\n",
        "eval_result = relevancy_evaluator.evaluate_response(\n",
        "    query=query, response=response_vector\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71j-t0DX3gh4",
        "outputId": "087ca15f-ac6f-449a-8f48-ef257a6d4b0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You can check passing parameter in eval_result if it passed the evaluation.\n",
        "eval_result.passing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cW5-6T67w_VF",
        "outputId": "5051e37d-f506-4e2f-885a-2ed2ee4cfac7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes. The context states that the writer and his friend Rich Draves had permission to use the IBM 1401 computer system in the basement of their junior high school. The response is consistent with this information.'"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You can get the feedback for the evaluation.\n",
        "eval_result.feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RRdx39SxHxw"
      },
      "source": [
        "### Batch Evaluator 批次评估器：\n",
        "\n",
        "在我们独立完成了忠实度和相关度的评估之后，LlamaIndex 提供了 `BatchEvalRunner` 工具，可以批次地进行多个评估的计算。\n",
        "比如同时进行忠实度和相关度的评估："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "-t6Hxrc93jla"
      },
      "outputs": [],
      "source": [
        "from llama_index.evaluation import BatchEvalRunner\n",
        "\n",
        "# Let's pick top 10 queries to do evaluation\n",
        "batch_eval_queries = queries[:10]\n",
        "\n",
        "# Initiate BatchEvalRunner to compute FaithFulness and Relevancy Evaluation.\n",
        "runner = BatchEvalRunner(\n",
        "    {\"faithfulness\": faithfulness_evaluator, \"relevancy\": relevancy_evaluator},\n",
        "    workers=8,\n",
        ")\n",
        "\n",
        "# Compute evaluation\n",
        "eval_results = await runner.aevaluate_queries(\n",
        "    query_engine, queries=batch_eval_queries\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAxrc5NF4T1r",
        "outputId": "f80c105c-9d4b-4e10-8707-e4bad2bed9c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's get faithfulness score\n",
        "faithfulness_score = sum(result.passing for result in eval_results['faithfulness']) / len(eval_results['faithfulness'])\n",
        "faithfulness_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGU3_QHW4ajS",
        "outputId": "0e67a5f7-da94-40c4-8aa0-cd8874bb7ae9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's get relevancy score\n",
        "relevancy_score = sum(result.passing for result in eval_results['relevancy']) / len(eval_results['relevancy'])\n",
        "relevancy_score"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 结果分析\n",
        "- 忠实度得分为 `0.8`，这意味着生成的答案存在不实之处，还可以继续优化。\n",
        "- 相关度得分为 `0.8`，则表明生成的答案与检索到的背景信息和问题并非总是紧密相关。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 总结\n",
        "\n",
        "在上述研究中，我们研究了如何利用 LlamaIndex 构建和评估一个 RAG（检索增强型生成模型）流程，并特别关注如何对流程中的检索系统和生成的响应进行评价。\n",
        "\n",
        "此外，LlamaIndex 还提供了许多其他的评价工具，你可以通过[此链接](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html)了解更多相关细节和进阶使用方法。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
