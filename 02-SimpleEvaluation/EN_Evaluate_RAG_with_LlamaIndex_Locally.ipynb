{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate RAG with LlamaIndex Locally"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Notes:\n",
        "> 1. I will use LM Studio to replace the OpenAPI API.\n",
        "> 2. The LLM Model is mistral-7b-instruct-v0.2.Q6_K"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SxQ2qzb7DPu1"
      },
      "source": [
        "In this notebook we will look into building an RAG pipeline and evaluating it with LlamaIndex. It has following 3 sections.\n",
        "\n",
        "1. Understanding Retrieval Augmented Generation (RAG).\n",
        "2. Building RAG with LlamaIndex.\n",
        "3. Evaluating RAG with LlamaIndex."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jYKmLpi0-AvJ"
      },
      "source": [
        "**Retrieval Augmented Generation (RAG)**\n",
        "\n",
        "LLMs are trained on vast datasets, but these will not include your specific data. Retrieval-Augmented Generation (RAG) addresses this by dynamically incorporating your data during the generation process. This is done not by altering the training data of LLMs, but by allowing the model to access and utilize your data in real-time to provide more tailored and contextually relevant responses.\n",
        "\n",
        "In RAG, your data is loaded and and prepared for queries or “indexed”. User queries act on the index, which filters your data down to the most relevant context. This context and your query then go to the LLM along with a prompt, and the LLM provides a response.\n",
        "\n",
        "Even if what you’re building is a chatbot or an agent, you’ll want to know RAG techniques for getting data into your application."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![RAG Overview](./images/llamaindex_rag_overview.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv9e2I1w-SLF"
      },
      "source": [
        "**Stages within RAG**\n",
        "\n",
        "There are five key stages within RAG, which in turn will be a part of any larger application you build. These are:\n",
        "\n",
        "**Loading:** this refers to getting your data from where it lives – whether it’s text files, PDFs, another website, a database, or an API – into your pipeline. LlamaHub provides hundreds of connectors to choose from.\n",
        "\n",
        "**Indexing:** this means creating a data structure that allows for querying the data. For LLMs this nearly always means creating vector embeddings, numerical representations of the meaning of your data, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.\n",
        "\n",
        "**Storing:** Once your data is indexed, you will want to store your index, along with any other metadata, to avoid the need to re-index it.\n",
        "\n",
        "**Querying:** for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies.\n",
        "\n",
        "**Evaluation:** a critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Besynnjg_Cg9"
      },
      "source": [
        "## Build RAG system.\n",
        "\n",
        "Now that we have understood the significance of RAG system, let's build a simple RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1NdWoBI_OFR"
      },
      "outputs": [],
      "source": [
        "# The nest_asyncio module enables the nesting of asynchronous functions within an already running async loop.\n",
        "# This is necessary because Jupyter notebooks inherently operate in an asynchronous loop.\n",
        "# By applying nest_asyncio, we can run additional async functions within this existing loop without conflicts.\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from llama_index.evaluation import generate_question_context_pairs\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
        "from llama_index.node_parser import SimpleNodeParser\n",
        "from llama_index.evaluation import generate_question_context_pairs\n",
        "from llama_index.evaluation import RetrieverEvaluator\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.embeddings import resolve_embed_model\n",
        "\n",
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7_DxX6xKUbH"
      },
      "source": [
        "#### Load Data and Build Index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vCJln_YKaK-",
        "outputId": "0004d366-35a3-49d4-806c-86a307aaa560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Pooling config file not found; pooling mode is defaulted to 'cls'.\n"
          ]
        }
      ],
      "source": [
        "# load data from data directory\n",
        "documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "\n",
        "# bge-m3 embedding model\n",
        "# https://huggingface.co/BAAI/bge-base-en-v1.5/tree/main\n",
        "embed_model = resolve_embed_model(\"local:BAAI/bge-base-en-v1.5\")\n",
        "\n",
        "# Load LM Studio LLM model\n",
        "llm = OpenAI(api_base=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n",
        "\n",
        "# Index the data\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    embed_model=embed_model, llm=llm,\n",
        ")\n",
        "\n",
        "# Transform data to Nodes struct\n",
        "node_parser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=128)\n",
        "nodes = node_parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# vetorize\n",
        "vector_index = VectorStoreIndex.from_documents(\n",
        "    documents, service_context=service_context\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65FyKpfY_inX"
      },
      "source": [
        "Build a QueryEngine and start querying."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "kOZBy--R_m3I"
      },
      "outputs": [],
      "source": [
        "query_engine = vector_index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "G7NVP-N4_rXF"
      },
      "outputs": [],
      "source": [
        "response_vector = query_engine.query(\"What did the author do growing up?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPYx0ycS_x9B"
      },
      "source": [
        "Check response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "BFFLl1d7_4r4",
        "outputId": "74c2bc81-f326-4981-c7ac-92eb836e5e99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"The author grew up in New Hampshire and spent most of his time reading science fiction and painting. He attended a boarding school in Massachusetts for high school, where he continued to paint and read. After graduating from college with a degree in computer science, he worked at various software companies before starting his own company, Viaweb, which was sold to Yahoo in 1996. He then moved to California and tried to focus on painting, but found it difficult due to lack of energy and motivation. He eventually returned to New York and resumed painting, this time with more success.\\n### Explanation:\\nThe author's childhood was marked by a love for reading science fiction and painting. He attended a boarding school in Massachusetts, where he continued to pursue these interests. After college, he worked in the software industry before starting his own company, Viaweb, which was sold to Yahoo in 1996. Following the sale of Viaweb, the author moved to California with the intention of focusing on painting, but found it difficult due to a lack of energy and motivation. He eventually returned to New York and resumed painting with more success.\""
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response_vector.response"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IfhTYwoO_8cd"
      },
      "source": [
        "By default it retrieves `two` similar nodes/ chunks. You can modify that in `vector_index.as_query_engine(similarity_top_k=k)`.\n",
        "\n",
        "Let's check the text in each of these retrieved nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcniGJVt_5V8",
        "outputId": "52b9d3da-30e7-460e-ab50-0ac3dc98ea1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\\n\\nI was puzzled by the 1401. I couldn\\'t figure out what to do with it. And in retrospect there\\'s not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn\\'t have any data stored on punched cards. The only other option was to do things that didn\\'t rely on any input, like calculate approximations of pi, but I didn\\'t know enough math to do anything interesting of that type. So I\\'m not surprised I can\\'t remember any programs I wrote, because they can\\'t have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn\\'t. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager\\'s expression made clear.\\n\\nWith microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then stopping. [1]\\n\\nThe first of my friends to get a microcomputer built it himself. It was sold as a kit by Heathkit. I remember vividly how impressed and envious I felt watching him sitting in front of it, typing programs right into the computer.\\n\\nComputers were expensive in those days and it took me years of nagging before I convinced my father to buy one, a TRS-80, in about 1980. The gold standard then was the Apple II, but a TRS-80 was good enough. This was when I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he\\'d write 2 pages at a time and then print them out, but it was a lot better than a typewriter.\\n\\nThough I liked programming, I didn\\'t plan to study it in college. In college I was going to study philosophy, which sounded much more powerful. It seemed, to my naive high school self, to be the study of the ultimate truths, compared to which the things studied in other fields would be mere domain knowledge. What I discovered when I got to college was that the other fields took up so much of the space of ideas that there wasn\\'t much left for these supposed ultimate truths. All that seemed left for philosophy were edge cases that people in other fields felt could safely be ignored.\\n\\nI couldn\\'t have put this into words when I was 18. All I knew at the time was that I kept taking philosophy courses and they kept being boring. So I decided to switch to AI.\\n\\nAI was in the air in the mid 1980s, but there were two things especially that made me want to work on it: a novel by Heinlein called The Moon is a Harsh Mistress, which featured an intelligent computer called Mike, and a PBS documentary that showed Terry Winograd using SHRDLU. I haven\\'t tried rereading The Moon is a Harsh Mistress, so I don\\'t know how well it has aged, but when I read it I was drawn entirely into its world. It seemed only a matter of time before we\\'d have Mike, and when I saw Winograd using SHRDLU, it seemed like that time would be a few years at most.'"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# First retrieved node\n",
        "response_vector.source_nodes[0].get_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Nor had I changed my grad student lifestyle significantly since we started. So when Yahoo bought us it felt like going from rags to riches. Since we were going to California, I bought a car, a yellow 1998 VW GTI. I remember thinking that its leather seats alone were by far the most luxurious thing I owned.\\n\\nThe next year, from the summer of 1998 to the summer of 1999, must have been the least productive of my life. I didn't realize it at the time, but I was worn out from the effort and stress of running Viaweb. For a while after I got to California I tried to continue my usual m.o. of programming till 3 in the morning, but fatigue combined with Yahoo's prematurely aged culture and grim cube farm in Santa Clara gradually dragged me down. After a few months it felt disconcertingly like working at Interleaf.\\n\\nYahoo had given us a lot of options when they bought us. At the time I thought Yahoo was so overvalued that they'd never be worth anything, but to my astonishment the stock went up 5x in the next year. I hung on till the first chunk of options vested, then in the summer of 1999 I left. It had been so long since I'd painted anything that I'd half forgotten why I was doing this. My brain had been entirely full of software and men's shirts for 4 years. But I had done this to get rich so I could paint, I reminded myself, and now I was rich, so I should go paint.\\n\\nWhen I said I was leaving, my boss at Yahoo had a long conversation with me about my plans. I told him all about the kinds of pictures I wanted to paint. At the time I was touched that he took such an interest in me. Now I realize it was because he thought I was lying. My options at that point were worth about $2 million a month. If I was leaving that kind of money on the table, it could only be to go and start some new startup, and if I did, I might take people with me. This was the height of the Internet Bubble, and Yahoo was ground zero of it. My boss was at that moment a billionaire. Leaving then to start a new startup must have seemed to him an insanely, and yet also plausibly, ambitious plan.\\n\\nBut I really was quitting to paint, and I started immediately. There was no time to lose. I'd already burned 4 years getting rich. Now when I talk to founders who are leaving after selling their companies, my advice is always the same: take a vacation. That's what I should have done, just gone off somewhere and done nothing for a month or two, but the idea never occurred to me.\\n\\nSo I tried to paint, but I just didn't seem to have any energy or ambition. Part of the problem was that I didn't know many people in California. I'd compounded this problem by buying a house up in the Santa Cruz Mountains, with a beautiful view but miles from anywhere. I stuck it out for a few more months, then in desperation I went back to New York, where unless you understand about rent control you'll be surprised to hear I still had my apartment, sealed up like a tomb of my old life. Idelle was in New York at least, and there were other people trying to paint there, even though I didn't know any of them.\\n\\nWhen I got back to New York I resumed my old life, except now I was rich. It was as weird as it sounds. I resumed all my old patterns, except now there were doors where there hadn't been. Now when I was tired of walking, all I had to do was raise my hand, and (unless it was raining) a taxi would stop to pick me up. Now when I walked past charming little restaurants I could go in and order lunch. It was exciting for a while. Painting started to go better. I experimented with a new kind of still life where I'd paint one painting in the old way, then photograph it and print it, blown up, on canvas, and then use that as the underpainting for a second still life, painted from the same objects (which hopefully hadn't rotted yet).\\n\\nMeanwhile I looked for an apartment to buy. Now I could actually choose what neighborhood to live in. Where, I asked myself and various real estate agents, is the Cambridge of New York? Aided by occasional visits to actual Cambridge, I gradually realized there wasn't one. Huh.\\n\\nAround this time, in the spring of 2000, I had an idea. It was clear from our experience with Viaweb that web apps were the future.\""
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Second retrieved node\n",
        "response_vector.source_nodes[1].get_text()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnvSrTeADNrX"
      },
      "source": [
        "We have built a RAG pipeline and now need to evaluate its performance. We can assess our RAG system/query engine using LlamaIndex's core evaluation modules. Let's examine how to leverage these tools to quantify the quality of our retrieval-augmented generation system."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jAMiOEsT_-o0"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Evaluation should serve as the primary metric for assessing your RAG application. It determines whether the pipeline will produce accurate responses based on the data sources and a range of queries.\n",
        "\n",
        "While it's beneficial to examine individual queries and responses at the start, this approach may become impractical as the volume of edge cases and failures increases. Instead, it may be more effective to establish a suite of summary metrics or automated evaluations. These tools can provide insights into overall system performance and indicate specific areas that may require closer scrutiny.\n",
        "\n",
        "In a RAG system, evaluation focuses on two critical aspects:\n",
        "\n",
        "*   **Retrieval Evaluation:** This assesses the accuracy and relevance of the information retrieved by the system.\n",
        "*   **Response Evaluation:** This measures the quality and appropriateness of the responses generated by the system based on the retrieved information."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "arGZqWQNIitt"
      },
      "source": [
        "#### Question-Context Pair Generation:\n",
        "\n",
        "For the evaluation of a RAG system, it's essential to have queries that can fetch the correct context and subsequently generate an appropriate response. `LlamaIndex` offers a `generate_question_context_pairs` module specifically for crafting questions and context pairs which can be used in the assessment of the RAG system of both Retrieval and Response Evaluation. For more details on Question Generation, please refer to the [documentation](https://docs.llamaindex.ai/en/stable/examples/evaluation/QuestionGeneration.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK0BWZ88LjDq",
        "outputId": "87d11e23-50cf-4ff3-a449-12264cfd8ab8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 46/46 [03:53<00:00,  5.08s/it]\n"
          ]
        }
      ],
      "source": [
        "qa_dataset = generate_question_context_pairs(\n",
        "    nodes,\n",
        "    llm=llm,\n",
        "    num_questions_per_chunk=20\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjs2cgTpOfLM"
      },
      "source": [
        "#### Retrieval Evaluation:\n",
        "\n",
        "We are now prepared to conduct our retrieval evaluations. We will execute our `RetrieverEvaluator` using the evaluation dataset we have generated.\n",
        "\n",
        "We first create the `Retriever` and then define two functions: `get_eval_results`, which operates our retriever on the dataset, and `display_results`, which presents the outcomes of the evaluation."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bE1Z77YyPNwE"
      },
      "source": [
        "Let's create the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "fV9IdnwLM_aw"
      },
      "outputs": [],
      "source": [
        "retriever = vector_index.as_retriever(similarity_top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLSNg2sSPc2U"
      },
      "source": [
        "Define `RetrieverEvaluator`. We use **Hit Rate** and **MRR** metrics to evaluate our Retriever.\n",
        "\n",
        "**Hit Rate:**\n",
        "\n",
        "Hit rate calculates the fraction of queries where the correct answer is found within the top-k retrieved documents. In simpler terms, it’s about how often our system gets it right within the top few guesses.\n",
        "\n",
        "**Mean Reciprocal Rank (MRR):**\n",
        "\n",
        "For each query, MRR evaluates the system’s accuracy by looking at the rank of the highest-placed relevant document. Specifically, it’s the average of the reciprocals of these ranks across all the queries. So, if the first relevant document is the top result, the reciprocal rank is 1; if it’s second, the reciprocal rank is 1/2, and so on.\n",
        "\n",
        "Let's check these metrics to check the performance of out retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "H6V_LCxrPQzp"
      },
      "outputs": [],
      "source": [
        "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
        "    [\"mrr\", \"hit_rate\"], retriever=retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYFgmnpRPX-x"
      },
      "outputs": [],
      "source": [
        "# Evaluate\n",
        "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3FLlvjoSbI5"
      },
      "source": [
        "Let's define a function to display the Retrieval evaluation results in table format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "S9T268MhRNxp"
      },
      "outputs": [],
      "source": [
        "def display_results(name, eval_results):\n",
        "    \"\"\"Display results from evaluate.\"\"\"\n",
        "\n",
        "    metric_dicts = []\n",
        "    for eval_result in eval_results:\n",
        "        metric_dict = eval_result.metric_vals_dict\n",
        "        metric_dicts.append(metric_dict)\n",
        "\n",
        "    full_df = pd.DataFrame(metric_dicts)\n",
        "\n",
        "    hit_rate = full_df[\"hit_rate\"].mean()\n",
        "    mrr = full_df[\"mrr\"].mean()\n",
        "\n",
        "    metric_df = pd.DataFrame(\n",
        "        {\"Retriever Name\": [name], \"Hit Rate\": [hit_rate], \"MRR\": [mrr]}\n",
        "    )\n",
        "\n",
        "    return metric_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "A1eESYN-RRgl",
        "outputId": "ff27adb0-d189-4b7d-8998-6df15b6a2014"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Retriever Name</th>\n",
              "      <th>Hit Rate</th>\n",
              "      <th>MRR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bge-m3 Embedding Retriever</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Retriever Name  Hit Rate  MRR\n",
              "0  bge-m3 Embedding Retriever       0.0  0.0"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "display_results(\"bge-m3 Embedding Retriever\", eval_results)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Observation:\n",
        "\n",
        "The Retriever with OpenAI Embedding  demonstrates a performance with a hit rate of `0.7586`, while the MRR, at `0.6206`, suggests there's room for improvement in ensuring the most relevant results appear at the top. The observation that MRR is less than the hit rate indicates that the top-ranking results aren't always the most relevant. Enhancing MRR could involve the use of rerankers, which refine the order of retrieved documents. For a deeper understanding of how rerankers can optimize retrieval metrics, refer to the detailed discussion in our [blog post](https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPxAHE3kSjsT"
      },
      "source": [
        "#### Response Evaluation:\n",
        "\n",
        "1. FaithfulnessEvaluator: Measures if the response from a query engine matches any source nodes which is useful for measuring if the response is hallucinated.\n",
        "2. Relevancy Evaluator: Measures if the response + source nodes match the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "-zMMJAQvRS8H"
      },
      "outputs": [],
      "source": [
        "# Get the list of queries from the above created dataset\n",
        "queries = list(qa_dataset.queries.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bNei9mj4UjN"
      },
      "source": [
        "#### Faithfulness Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mA3NAKevLMC"
      },
      "source": [
        "Let's start with FaithfulnessEvaluator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3ITPhWVrjvP"
      },
      "source": [
        "I will use `mistral-7b-instruct-v0.2.Q6_K` for generating response for a given query and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "lrrD5n6w3Oet"
      },
      "outputs": [],
      "source": [
        "vector_index = VectorStoreIndex(nodes, service_context = service_context)\n",
        "query_engine = vector_index.as_query_engine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UvtdrdovgZ_"
      },
      "source": [
        "Create a  FaithfulnessEvaluator.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "dbvXvcFnU09s"
      },
      "outputs": [],
      "source": [
        "from llama_index.evaluation import FaithfulnessEvaluator\n",
        "faithfulness_raven13b = FaithfulnessEvaluator(service_context=service_context)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uA1hQ6_F4NoA"
      },
      "source": [
        "Let's evaluate on one question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "9lfyhUuDz6cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Where did the writer have permission to use the IBM 1401 computer system?'"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_query = queries[3]\n",
        "eval_query"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate response first and use faithfull evaluator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "response_vector = query_engine.query(eval_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "MZ6lvmRf3j8i"
      },
      "outputs": [],
      "source": [
        "# Compute faithfulness evaluation\n",
        "eval_result = faithfulness_raven13b.evaluate_response(response=response_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj79Rq-gn3cv",
        "outputId": "5078aeb7-c620-45d6-dc1f-215e716f4e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You can check passing parameter in eval_result if it passed the evaluation.\n",
        "eval_result.passing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuVReAjp4eZ_"
      },
      "source": [
        "#### Relevancy Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhomR2dP1Ybf"
      },
      "source": [
        "RelevancyEvaluator is useful to measure if the response and source nodes (retrieved context) match the query. Useful to see if response actually answers the query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13nay3W6tyqj"
      },
      "source": [
        "Instantiate `RelevancyEvaluator` for relevancy evaluation with `gpt-4`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "Qw5X_hMB24kC"
      },
      "outputs": [],
      "source": [
        "from llama_index.evaluation import RelevancyEvaluator\n",
        "\n",
        "relevancy_raven13b = RelevancyEvaluator(service_context=service_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIl_JJhFvNhu"
      },
      "source": [
        "Let's do relevancy evaluation for one of the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Where did the writer have permission to use the IBM 1401 computer system?'"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Pick a query\n",
        "query = queries[3]\n",
        "query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "r9FwcImG3cV0"
      },
      "outputs": [],
      "source": [
        "# Generate response.\n",
        "# response_vector has response and source nodes (retrieved context)\n",
        "response_vector = query_engine.query(query)\n",
        "\n",
        "# Relevancy evaluation\n",
        "eval_result = relevancy_raven13b.evaluate_response(\n",
        "    query=query, response=response_vector\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71j-t0DX3gh4",
        "outputId": "087ca15f-ac6f-449a-8f48-ef257a6d4b0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You can check passing parameter in eval_result if it passed the evaluation.\n",
        "eval_result.passing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cW5-6T67w_VF",
        "outputId": "5051e37d-f506-4e2f-885a-2ed2ee4cfac7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes. The context states that the writer and his friend Rich Draves had permission to use the IBM 1401 computer system in the basement of their junior high school. The response is consistent with this information.'"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You can get the feedback for the evaluation.\n",
        "eval_result.feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RRdx39SxHxw"
      },
      "source": [
        "#### Batch Evaluator:\n",
        "\n",
        "Now that we have done FaithFulness and Relevancy Evaluation independently. LlamaIndex has `BatchEvalRunner` to compute multiple evaluations in batch wise manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "-t6Hxrc93jla"
      },
      "outputs": [],
      "source": [
        "from llama_index.evaluation import BatchEvalRunner\n",
        "\n",
        "# Let's pick top 10 queries to do evaluation\n",
        "batch_eval_queries = queries[:10]\n",
        "\n",
        "# Initiate BatchEvalRunner to compute FaithFulness and Relevancy Evaluation.\n",
        "runner = BatchEvalRunner(\n",
        "    {\"faithfulness\": faithfulness_raven13b, \"relevancy\": relevancy_raven13b},\n",
        "    workers=8,\n",
        ")\n",
        "\n",
        "# Compute evaluation\n",
        "eval_results = await runner.aevaluate_queries(\n",
        "    query_engine, queries=batch_eval_queries\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAxrc5NF4T1r",
        "outputId": "f80c105c-9d4b-4e10-8707-e4bad2bed9c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's get faithfulness score\n",
        "faithfulness_score = sum(result.passing for result in eval_results['faithfulness']) / len(eval_results['faithfulness'])\n",
        "faithfulness_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGU3_QHW4ajS",
        "outputId": "0e67a5f7-da94-40c4-8aa0-cd8874bb7ae9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's get relevancy score\n",
        "relevancy_score = sum(result.passing for result in eval_results['relevancy']) / len(eval_results['relevancy'])\n",
        "relevancy_score\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Observation:\n",
        "\n",
        "Faithfulness score of `0.8` signifies that the generated answers contain little hallucinations and are entirely based on retrieved context.\n",
        "\n",
        "Relevancy score of `0.8` suggests that the answers generated are not consistently aligned with the retrieved context and the queries."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vzjCqAIeRrk1"
      },
      "source": [
        "In this notebook, we have explored how to build and evaluate a RAG pipeline using LlamaIndex, with a specific focus on evaluating the retrieval system and generated responses within the pipeline. \n",
        "\n",
        "LlamaIndex offers a variety of other evaluation modules as well, which you can explore further [here](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
